{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm, trange\n",
    "from math import exp, sqrt, log\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.special import i0, i1, iv\n",
    "from numpy import random\n",
    "from torch.nn.functional import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The setup of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BASICnet(nn.Module):\n",
    "    def __init__(self, dim, width):\n",
    "        super(BASICnet, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.width = width\n",
    "\n",
    "        self.fc1 = nn.Linear(self.dim, self.width)\n",
    "        self.fc2 = nn.Linear(self.width, self.width)\n",
    "        self.fc3 = nn.Linear(self.width, self.width)\n",
    "        self.fc4 = nn.Linear(self.width, self.width)\n",
    "\n",
    "        self.output = nn.Linear(self.width, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # s = nn.ConstantPad2d(x, (0, self.dim - self.dim))\n",
    "        y = self.fc1(x)\n",
    "        y = torch.tanh(y)\n",
    "        y = self.fc2(y)\n",
    "        y = torch.tanh(y)\n",
    "        y = self.fc3(y)\n",
    "        y = torch.tanh(y)\n",
    "        y = self.fc4(y)\n",
    "        y = torch.tanh(y)\n",
    "        \n",
    "        output = self.output(y)\n",
    "        return output\n",
    "\n",
    "    def assign_value(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                nn.init.constant_(m.bias.data, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cauculate equation (Righthand side of the equation, RHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fx(x):\n",
    "    pi = torch.tensor(np.pi, dtype=torch.float32)\n",
    "    f_x = -160 * pi**2 * torch.prod(torch.sin(4 * pi * x), dim=-1)\n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the advice calculate_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fx(X):\n",
    "    # Check if there are boundary cases with 1 and -1\n",
    "    has_boundary = torch.any((X == -1) | (X == 1))\n",
    "\n",
    "    if has_boundary:\n",
    "        # If there are boundary cases, set fx to zero\n",
    "        fx = torch.tensor(0.0, dtype=torch.float32)\n",
    "    else:\n",
    "        # Calculate the regular fx\n",
    "        fx = -160 * (torch.tensor(4 * np.pi, dtype=torch.float32).pow(2)) * torch.prod(torch.sin(4 * np.pi * X), dim=-1)\n",
    "\n",
    "    return fx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate model from the dataset and using montecarlo method to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def data_generator_monte_carlo(Numberofgenpoints, dim, prob_special = 0.1):\n",
    "    source = torch.randn(size = (Numberofgenpoints, dim)) # 生成一个大矩阵\n",
    "    source = normalize(source, p=2)\n",
    "    radius = torch.rand(size = (Numberofgenpoints, 1))\n",
    "    radius = torch.pow(torch.rand(size = (Numberofgenpoints, 1)), 1/dim)\n",
    "    source = source * radius\n",
    "\n",
    "    # 随即确认替换的行\n",
    "    num_replace_incol = torch.randint(1, Numberofgenpoints, size=(1,)).item()\n",
    "\n",
    "    for _ in range(num_replace_incol):\n",
    "        if random.random() < prob_special:\n",
    "            special_value = random.choice([1, -1])\n",
    "            row_index = torch.randint(0, Numberofgenpoints, size=(1,)).item()\n",
    "            source[row_index] = torch.full((dim, ), special_value)\n",
    "        else:\n",
    "            row_index = torch.randint(0, Numberofgenpoints, size=(1,)).item()\n",
    "            col_index = torch.randint(0, dim, size=(1,)).item()\n",
    "            replace_value = torch.randint(0, 2, size=(1,)).item() * 2 - 1\n",
    "            source[row_index, col_index] = replace_value\n",
    "\n",
    "    max_value = torch.max(source)\n",
    "    min_value = torch.min(source)\n",
    "\n",
    "    if max_value > 1 or min_value < -1:\n",
    "        raise ValueError\n",
    "    return source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已经生成了包括边界条件的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "GenPointsNum = 10000\n",
    "Dim = 10\n",
    "source = data_generator_monte_carlo(GenPointsNum, Dim, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, weight=1.0):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "\n",
    "    def forward(self, y_pred, X):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            u = y_pred\n",
    "            u_x = torch.autograd.grad(u, X, torch.ones_like(u), create_graph=True)[0]\n",
    "            u_xx = torch.autograd.grad(u_x, X, torch.ones_like(u_x), create_graph=True)[0]\n",
    "            fx = calculate_fx(X)\n",
    "            fx = torch.tensor(fx, dtype=torch.float32)\n",
    "\n",
    "            # 计算 PDE 损失\n",
    "            pde_loss = F.mse_loss(u_xx.sum(dim=1), fx)\n",
    "\n",
    "            # 计算边界损失\n",
    "            boundary_loss = F.mse_loss(u, torch.zeros_like(u))\n",
    "\n",
    "            # 计算均方差损失(Mean Absolute Error)\n",
    "            mae = F.l1_loss(u_xx.sum(dim=1), fx)\n",
    "\n",
    "            # 总损失\n",
    "            total_loss = pde_loss + boundary_loss + 0.5 * mae\n",
    "\n",
    "            return total_loss\n",
    "\n",
    "loss_fn = CustomLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757501\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 500]           5,500\n",
      "            Linear-2                  [-1, 500]         250,500\n",
      "            Linear-3                  [-1, 500]         250,500\n",
      "            Linear-4                  [-1, 500]         250,500\n",
      "            Linear-5                    [-1, 1]             501\n",
      "================================================================\n",
      "Total params: 757,501\n",
      "Trainable params: 757,501\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 2.89\n",
      "Estimated Total Size (MB): 2.90\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 10000\n",
    "BATCH_SIZE = 1000\n",
    "loss_list = np.zeros(NUM_EPOCHS)\n",
    "test_loss = np.zeros(NUM_EPOCHS)\n",
    "max_value_loss = np.zeros(NUM_EPOCHS)\n",
    "min_value_loss = np.zeros(NUM_EPOCHS)\n",
    "\n",
    "net = BASICnet(dim=Dim, width=500)\n",
    "\n",
    "# Optimizer and scheduler define there\n",
    "Optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "Scheduler = StepLR(Optimizer, step_size=1000, gamma=0.8)\n",
    "\n",
    "# orignal print\n",
    "print(sum(p.numel() for p in net.parameters() if p.requires_grad))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "summary(net, (Dim,))\n",
    "\n",
    "data_loader = DataLoader(source, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors does not require grad",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/reaganwu/Projects/MonteCarlo_Reagan/Reference/PINN.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/reaganwu/Projects/MonteCarlo_Reagan/Reference/PINN.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m Optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reaganwu/Projects/MonteCarlo_Reagan/Reference/PINN.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m y_pred \u001b[39m=\u001b[39m net(batch)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/reaganwu/Projects/MonteCarlo_Reagan/Reference/PINN.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_pred, batch)  \u001b[39m# 不再传递y_true\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reaganwu/Projects/MonteCarlo_Reagan/Reference/PINN.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reaganwu/Projects/MonteCarlo_Reagan/Reference/PINN.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m Optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/pntorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/reaganwu/Projects/MonteCarlo_Reagan/Reference/PINN.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reaganwu/Projects/MonteCarlo_Reagan/Reference/PINN.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reaganwu/Projects/MonteCarlo_Reagan/Reference/PINN.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     u \u001b[39m=\u001b[39m y_pred\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/reaganwu/Projects/MonteCarlo_Reagan/Reference/PINN.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     u_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(u, X, torch\u001b[39m.\u001b[39;49mones_like(u), create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reaganwu/Projects/MonteCarlo_Reagan/Reference/PINN.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     u_xx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(u_x, X, torch\u001b[39m.\u001b[39mones_like(u_x), create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/reaganwu/Projects/MonteCarlo_Reagan/Reference/PINN.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     fx \u001b[39m=\u001b[39m calculate_fx(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/pntorch/lib/python3.8/site-packages/torch/autograd/__init__.py:234\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    235\u001b[0m     outputs, grad_outputs_, retain_graph, create_graph,\n\u001b[1;32m    236\u001b[0m     inputs, allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors does not require grad"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import trange\n",
    "\n",
    "# 其他定义和初始化\n",
    "\n",
    "for epoch in trange(NUM_EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device)  # 将数据移到GPU上，如果可用\n",
    "        Optimizer.zero_grad()\n",
    "        y_pred = net(batch)\n",
    "        loss = loss_fn(y_pred, batch)  # 不再传递y_true\n",
    "        loss.backward()\n",
    "        Optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(data_loader)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pntorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
